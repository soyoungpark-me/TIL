# Lucene 기초 다지기

 **출처 :** [**실전비급 아파치 루씬 7: 엘라스틱서치 검색 엔진을 향한 첫걸음**](https://book.naver.com/bookdb/book_detail.nhn?bid=14134564)

#### 목차

1. [루씬의 이해](https://github.com/3457soso/TIL/blob/master/Lucene/01_Intro.md)
2. [텍스트 색인](https://github.com/3457soso/TIL/blob/master/Lucene/02_index.md)
3. **텍스트 분석**
   - [**분석기 이해**](#1-분석기-이해)
   - [**문장을 음절로 분석하기**](#2-문장을-음절로-분석하기)
   - [**루씬의 기본 분석기**](#3-루씬의-기본-분석기)
   - [**확장 분석기**](#4-확장-분석기)
4. [텍스트 검색과 질의 방법](https://github.com/3457soso/TIL/blob/master/Lucene/04_query.md)
5. [루씬의 고급 검색](https://github.com/3457soso/TIL/blob/master/Lucene/05_core.md)
6. [루씬 동작 방식 이해하기](https://github.com/3457soso/TIL/blob/master/Lucene/06_inside.md)
7. [다양한 확장 기능](https://github.com/3457soso/TIL/blob/master/Lucene/07_extensions.md)



___

## 텍스트 분석

### 1. 분석기 이해 

- 분석기에서 텍스트 처리는 **색인과 검색 과정 모두** 에서 일어난다
- 분석기는 루씬의 핵심 기능으로, 언어별로 다양한 분석기가 제공된다.

#### 1) 색인 과정에서의 분석

- 루씬에서 분석은 **텍스트를 형태소로 만들어 검색에 용이하게 만드는 과정** 
  - **형태소** : 뜻을 가진 가장 작은 말의 단위
- 색인 과정에서의 분석은 **색인 시 텍스트 데이터를 검색에 용이한 형태로 저장하는 것**
  - 텍스트를 쪼개고 변환해 **텀을 만드는 분석 과정** 이 필수다!
- **데이터 정제**
  - 텍스트 분석 과정에 앞서 1차 데이터 정제 작업이 필요하다.
  - Ex) `CharFilter` : 입력 문자를 사전에 처리하는 구성 요소임!
- **역색인 구조로 저장**
  - 루씬은 데이터를 저장할 때, 검색 속도를 높이고자 **내부에 사전을 만든다**
  - **토큰화** : 토크나이저에 의해 단어를 쪼개는 과정
  - **토큰스트림 (TokenStream)** : 토크나이저가 쪼갠 일련의 단어 흐름
  - **토큰 필터** : 검색에 맞춰 단어를 가공하는 일
  - **텀** : 이런 과정을 거쳐 생성된 **가공된 단어**

#### 2) 검색에서의 분석

- 검색에 용이하도록 **사용자 질의도** 색인처럼 변환하고 텍스트를 분석한다.
- 검색 시에는 사용자 질의가 문서의 텀에 최대한 일치하도록, 
  - 검색어를 단어로 쪼개는 **토큰화**를 하고, **토큰 필터** 를 사용해 질의를 가공한다.

#### 3) 분석 과정

`Analyzer` 클래스는 **인덱싱 및 검색 프로세스에서 사용하는 텍스트를 분석** 한다.

1. **분석기** : 텍스트 분석 결과로 `TokenStream` 객체를 생성
2. **토큰화 과정** : 토크나이저와 토큰 필터를 사용해 텍스트를 **의미가 있는 단어인 토큰** 으로 나눈다
   - **하는 일**
     - **동사원형화 (Stemming)** : likes → like
     - **불용어 필터링 (Stop Words Filtering)** : the, and, a와 같은 불용어 필터링
     - **텍스트 정규화** : 악센트 같은 기타 문자 제거
     - **동의어 확장** : 해당 단어의 동의어를 포함한 문자로 함께 조회하도록...
   - **토크나이저** : 텍스트를 토큰으로 분할해 쪼갠다.
   - **토큰 필터** : 토크나이저로부터 만들어진 토큰을 토큰 필터로 변형해 **텀 생성**
     - **[주의]** *토큰 필터에 따라 단어가 변형되므로 토큰 필터의 적용 순서가 결과에 영향을 미침*

<br>

___

### 2. 문장을 음절로 분석하기

#### 1) TokenStream 클래스

- 문서의 필드나 쿼리의 토큰을 **순서대로 분석하는 객체**
- 토큰화와 토큰 필터 과정에서 **생성되고 재사용 됨**

#### 2) AttributeSource 클래스

- **토큰의 속성 모음** 으로, 다음과 같은 속성들을 저장한다
  - `CharTermAttribute` : 토큰의 텍스트
  - `OffsetAttribute` : 문자로 표시된 토큰의 시작과 끝 오프셋
  - `PositionIncrementAttribute` : 토큰의 위치 값으로, 토큰을 읽으면 값이 증가함
  - `TypeAttribute` : 토큰의 유형 (기본 값은 단어)
- **토큰 위칫값 증가는?**
  - 토큰 위칫값은 **현재 읽어야 할 토큰의 위치** 를 알려준다.
  - 해당 토큰을 읽고 난 후 or 토큰 필터에서 특정 토큰의 값을 제거한 후에 증가시켜주면 된다.

#### 3) Tokenizer 클래스

- 전체 텍스트를 **단어로 분리** 하는 클래스이다
- **주요 토크나이저**
  - `StandardTokenizer` : 문법 기반 토크나이저로, 유니코드 텍스트 분할 알고리즘에 명시된 규칙을 따른다
  - `WhitespaceTokenizer` : **공백 문자** 로 텍스트 분할
  - `NGramTokenizer` : **N-Gram** 모델을 사용해 분할
  - `EdgeNGramTokenizer` : 입력 토큰의 시작 부분에서부터 **N-Gram** 생성
  - `LowerCaseTokenizer` : 영문을 분할하는 토크나이저로, **대문자를 소문자로 변환**

#### 4) TokenFilter 클래스

- 토크나이저에 의해 분리된 토큰을 **변경하거나 제거**
- **주요 토큰 필터**
  - `StandardFilter` : `StandardTokenizer`로 추출한 토큰 표준화
  - `LowerCaseFilter` : 토큰의 텍스트를 **소문자**로 표준화
  - `StopFilter` : 토큰스트림에서 **불용어 제거**
  - `CharFilter` : 특정 데이터를 변환하는 추상 클래스

#### 5) Analyzer 클래스

- 추상 클래스로, 공장처럼 **텍스트 분석 과정 일체** 를 생성함!
- 이 과정에서 토큰의 정보(스트림)을 가지는 `TokenStream`을 생성한다.
  - 생성된 `TokenStream`을 어떻게 분할하고 조작할지를 정의하는 것이 주요 역할!
- **동작 흐름**
  1. 토큰의 구성 요소인 `TokenStreamComponents`를 생성하기 위한 `createComponents()` 메소드 구현
  2. `createComponents()`에서 토큰화 과정을 담당하는 토크나이저와 토큰 필터 지정
  3. 토큰화와 토큰 필터 과정에서 만들어진 `TokenStreamComponents` 객체는 `TokenStream`에서 재사용
- **[TIP]** *분석기의 종류 선택과 순서는 매우 중요하며, 너무 많은 분석을 하면 색인 성능이 떨어진다*
- **[TIP]** *인덱스 생성과 검색에는 가급적 같은 분석기를 사용하자* 
  - 예외 경우 1) 특정 검색어로 인해 더 많은 단어를 필터링해야 하는 경우
  - 예외 경우 2) 동의어, 두 문자어, 자동 맞춤법 교정 등으로 검색어를 확장한 경우

<br>

___

### 3. 루씬의 기본 분석기

#### 1) StandardAnalyzer

- 이메일, 주소, 약자, 중국어, 한국어, 영문 등의 텍스트를 **정교화된 문법으로 분할**
- 분할은 **유니코드 텍스트 분할 알고리즘** 사용
- `StandardTokenizer` -----→ `StandardFilter` → `LowerCaseFilter` → `StopFilter`

#### 2) SimpleAnalyzer

- 문자가 아닌 것을 쪼개고, 대문자를 소문자로 바꾼다.
- 영문과 달리 아시아권 문자는 단어를 공백 단위로만 쪼갤 수 없어 큰 의미 X
- `LowerCaseTokenizer` -----→ `LetterFilter` → `LowerCaseFilter`

#### 3) WhitespaceAnalyzer

- 공백을 제거하는 분석기로, 내부적으로는 **유니코드 포인트에서 공백이라 정의한 것**을 제거 (Ex. 스페이스바, 탭)

#### 4) StopAnalyzer

- 역할은 **불용어 제거**
- 불용어 처리에는 추가적으로 직접 **불용어 사전** 을 생성해 사용할 수 있음

#### 5) KeywordAnalyzer

- 텍스트를 분할하지 않고 **하나의 토큰으로 취급**

<br>

___

### 4. 확장 분석기

#### 1) 동의어 처리

- 유의어, 동의어, 별명은 **루씬의 토큰 필터로 처리**
- 동의어 처리의 기본 전략은 → 인덱스 텀에 유의해 **별명을 함께 등록**, 일관된 결과를 내놓는 것!
  1. 인덱스 **생성** 시 텀과 동의어를 함께 구성
  2. **검색** 시 동의어를 활용해 처리

#### 2) N-Gram 분석

- 영문의 경우 공백으로 단어를 분리해도 되지만, 한글은 **조사**같은 것 때매 공백으로만 분리할 수 X

- **N-Gram**은 통계와 확률을 바탕으로 둔 알고리즘으로, 텍스트 분석 분야에서 널리 쓰인다.

  - 대상이 되는 문장을 기준으로 N개의 문자열을 왼쪽에서 오른쪽으로 한 자씩 움직이며 문자열 생성

  - 검색어를 자르는 용도보다 **특정 패턴을 분석** 하는 데 주로 사용

    → 한글 분석과 검색에는 **형태소 분석기** 가 주로 사용됨

#### 3) 언어별 형태소 분석기

- 언어의 특성을 반영해 색인이나 검색을 하기 위해 사용한다.
- **형태소** : (어휘적 + 문법적) 의미를 가지는 *가장 작은 말의 단위*
- **형태소 분석** : 형태소보다 단위가 큰 언어 단위인 어절, 혹은 문장을 **최소 의미 단위인 형태소로 분할** 하는 것!
- **쓰는 이유**
  - 형태소 분석으로 **실질적인 의미를 가지는 형태소만 색인하고 검색** 하면
  - **N-Gram**으로 무조건 N개로 쪼갠 결과를 색인하는 것보다 **검색이 효율적**
- 각 언어마다 문장을 구성하는 형태소가 달라, 형태소 분석은 **국가별 언어에 특화** 된다
- **한글 형태소 분석기**
  - 아리랑, 꼬꼬마, 은전 한닢, **노리**